{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mohdn\\Anaconda3\\lib\\site-packages\\nltk\\twitter\\__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
      "  warnings.warn(\"The twython library has not been installed. \"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from statistics import mean \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Web scarapping \n",
    "\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "#Error Handling \n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "os.chdir('C:/Users/mohdn/OneDrive/Desktop/Nadeem/DataSets/Alexa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "dataframe1=pd.DataFrame()\n",
    "print(dataframe1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "6\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "j=6\n",
    "e='Exception message : HTTP Error 503: Service Unavailable'\n",
    "while(j<447 and e==('Exception message : HTTP Error 503: Service Unavailable')):\n",
    "    try:\n",
    "        for i in range(j,302):\n",
    "            print(i)\n",
    "            #447#India : html = urlopen(\"https://www.amazon.in/All-new-Echo-Dot-3rd-Gen/product-reviews/B0792KTHKK/ref=cm_cr_getr_d_paging_btm_next_{}?showViewpoints=1&pageNumber={}&reviewerType=all_reviews\".format(i,i))\n",
    "            #US :html = urlopen(\"https://www.amazon.com/Echo-Dot-3rd-Gen-Sandstone/product-reviews/B0792R1RSN/ref=cm_cr_arp_d_paging_btm_next_{}?ie=UTF8&reviewerType=all_reviews&pageNumber={}\".format(i,i))\n",
    "            html = urlopen(\"https://www.amazon.ca/All-new-Echo-Dot-3rd-gen/product-reviews/B0792JYXZK/ref=cm_cr_arp_d_paging_btm_next_{}?showViewpoints=1&pageNumber={}\".format(i,i))\n",
    "            soup = BeautifulSoup(html,\"lxml\")\n",
    "            html = soup.prettify('utf-8')\n",
    "\n",
    "            names = []\n",
    "            shor_reviews=[]\n",
    "            #usefullness = []\n",
    "            dates = []\n",
    "\n",
    "            for span in soup.findAll('span',attrs={'class': 'a-profile-name'}):\n",
    "                name = span.text.strip()\n",
    "                names.append(name)\n",
    "            names =names [2::1]\n",
    "\n",
    "            for span in soup.findAll('span',attrs={'class': 'cr-original-review-content'}):\n",
    "                short_review = span.text.strip()\n",
    "                shor_reviews.append(short_review)\n",
    "\n",
    "        #for span in soup.findAll('span',attrs={'class': 'a-size-base a-color-tertiary cr-vote-text'}):\n",
    "        #    use = span.text.strip()\n",
    "        #    usefullness.append(use)\n",
    "\n",
    "\n",
    "            for span in soup.findAll('span',attrs={'class': 'a-size-base a-color-secondary review-date'}):\n",
    "                date = span.text.strip()\n",
    "                dates.append(date)\n",
    "\n",
    "            dates =dates [2::1]\n",
    "\n",
    "            long_reviews = shor_reviews[1::2]\n",
    "            short_reviews = shor_reviews[0::2]\n",
    "\n",
    "            dataframe = pd.DataFrame({'Names':names,'Short_Reviews':short_reviews,'Reviews':long_reviews,'Date':dates})\n",
    "\n",
    "            dataframe1 = dataframe1.append(dataframe,ignore_index=True)\n",
    "\n",
    "            i=i+1\n",
    "\n",
    "    except BaseException as ex:\n",
    "        # Get current system exception\n",
    "        ex_type, ex_value, ex_traceback = sys.exc_info()\n",
    "\n",
    "        # Extract unformatter stack traces as tuples\n",
    "        trace_back = traceback.extract_tb(ex_traceback)\n",
    "\n",
    "        # Format stacktrace\n",
    "        stack_trace = list()\n",
    "\n",
    "        for trace in trace_back:\n",
    "            stack_trace.append(\"File : %s , Line : %d, Func.Name : %s, Message : %s\" % (trace[0], trace[1], trace[2], trace[3]))\n",
    "\n",
    "        #print(\"Exception type : %s \" % ex_type.__name__)\n",
    "        #print(\"Exception message : %s\" %ex_value)\n",
    "        #print(\"Stack trace : %s\" %stack_trace)\n",
    "        e = (\"Exception message : %s\" %ex_value)\n",
    "    j=i\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Exception message : arrays must all be same length'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exporting CSV files\n",
    "export_csv_india = dataframe1.to_csv(r'C:\\Users\\mohdn\\OneDrive\\Desktop\\Nadeem\\DataSets\\Alexa\\Alexa_Reviews_India.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Reading the files\n",
    "alexa = pd.read_csv('Alexa.csv')\n",
    "\n",
    "stopword = nltk.corpus.stopwords.words('english')\n",
    "ps = nltk.PorterStemmer()\n",
    "wn=nltk.WordNetLemmatizer()\n",
    "sid = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#obtaining the sentiment scores for the reviews.\n",
    "\n",
    "scores = []\n",
    "for review in (alexa['Review']):\n",
    "    score = sid.polarity_scores(str(review))\n",
    "    scores.append(score)\n",
    "scores = pd.DataFrame.from_dict(scores)\n",
    "\n",
    "country_wise = alexa[['Country','Review','Ratings']]\n",
    "country_wise = pd.concat([country_wise,scores],axis=1)\n",
    "\n",
    "country_wise.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Overall mean\n",
    "\n",
    "comp_mean = country_wise['compound'].mean()\n",
    "neg_mean = country_wise['neg'].mean()\n",
    "neu_mean = country_wise['neu'].mean()\n",
    "pos_mean = country_wise['pos'].mean()\n",
    "\n",
    "list_val = [comp_mean,neg_mean,neu_mean,pos_mean]\n",
    "list_names = ['comp_mean','neg_mean','neu_mean','pos_mean']\n",
    "\n",
    "list_df = pd.DataFrame({'name':li_names,\n",
    "                        'values':li})\n",
    "\n",
    "print(plt.bar(list_names,list_val, align='center', alpha=0.5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Country wise Mean\n",
    "\n",
    "country = [\"Canada\",\"Australia\",\"UK\"]\n",
    "rating=[]\n",
    "compound=[]\n",
    "neg=[]\n",
    "neu=[]\n",
    "pos=[]\n",
    "\n",
    "for item in country:\n",
    "    \n",
    "    \n",
    "    subset = country_wise[country_wise['Country']==item]\n",
    "    c_rat ='Mean_Ratings_{}'.format(item)\n",
    "    avg_rat= str(subset['Ratings'].mean())\n",
    "    print('\\n{} is {}'.format(c_rat,avg_rat))\n",
    "    rating.append(avg_rat)\n",
    "    \n",
    "    c_comp ='Mean_Compound_{}'.format(item)\n",
    "    avg_comp= str(subset['compound'].mean())\n",
    "    print('{} is {}'.format(c_comp,avg_comp))\n",
    "    compound.append(avg_comp)\n",
    "    \n",
    "    c_neg ='Mean_Negativity_{}'.format(item)\n",
    "    avg_neg= str(subset['neg'].mean())\n",
    "    print('{} is {}'.format(c_neg,avg_neg))\n",
    "    neg.append(avg_neg)\n",
    "    \n",
    "    c_neu ='Mean_Neutral_{}'.format(item)\n",
    "    avg_neu= str(subset['neu'].mean())\n",
    "    print('{} is {}'.format(c_neu,avg_neu))\n",
    "    neu.append(avg_neu)\n",
    "    \n",
    "    c_pos ='Mean_Postivity_{}'.format(item)\n",
    "    avg_pos= str(subset['pos'].mean())\n",
    "    print('{} is {}'.format(c_pos,avg_pos))\n",
    "    pos.append(avg_pos)\n",
    "    \n",
    "\n",
    "print(rating) \n",
    "\n",
    "\n",
    "\n",
    "new_list = [compound,pos,neg,neu]\n",
    "i=1\n",
    "for item in new_list:\n",
    "    plt.subplot(4,1,i)\n",
    "    plt.bar(country,item, align='center', alpha=0.5)\n",
    "    i=i+1\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text =\"\".join([char for char in text if char not in string.punctuation])\n",
    "    token = re.split('\\W+',text)\n",
    "    stop_word = [word for word in token if word not in stopword]\n",
    "    #text = [ps.stem(word) for word in stop_word]\n",
    "    text = [wn.lemmatize(word) for word in stop_word]\n",
    "    return text\n",
    "\n",
    "alexa['Clean_Review'] = alexa['Review'].apply(lambda x: clean_text(x.lower()))\n",
    "#alexa['Short_Clean_Review'] = alexa['Short Review'].apply(lambda x: clean_text(x.lower()))\n",
    "alexa.head()  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DEBUGGING AREA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(97,100):\n",
    "    print(i)\n",
    "    html = urlopen(\"https://www.amazon.in/All-new-Echo-Dot-3rd-Gen/product-reviews/B0792KTHKK/ref=cm_cr_getr_d_paging_btm_next_{}?showViewpoints=1&pageNumber={}&reviewerType=all_reviews\".format(i,i))\n",
    "    soup = BeautifulSoup(html,\"lxml\")\n",
    "    html = soup.prettify('utf-8')\n",
    "    \n",
    "    names = []\n",
    "    shor_reviews=[]\n",
    "    #usefullness = []\n",
    "    dates = []\n",
    "\n",
    "    for span in soup.findAll('span',attrs={'class': 'a-profile-name'}):\n",
    "        name = span.text.strip()\n",
    "        names.append(name)\n",
    "    names =names [2::1]\n",
    "\n",
    "    for span in soup.findAll('span',attrs={'class': 'cr-original-review-content'}):\n",
    "        short_review = span.text.strip()\n",
    "        shor_reviews.append(short_review)\n",
    "    \n",
    "    #for span in soup.findAll('span',attrs={'class': 'a-size-base a-color-tertiary cr-vote-text'}):\n",
    "    #    use = span.text.strip()\n",
    "    #    usefullness.append(use)\n",
    "    \n",
    "    for span in soup.findAll('span',attrs={'class': 'a-size-base a-color-secondary review-date'}):\n",
    "        date = span.text.strip()\n",
    "        dates.append(date)\n",
    "\n",
    "    dates =dates [2::1]\n",
    "\n",
    "    long_reviews = shor_reviews[1::2]\n",
    "    short_reviews = shor_reviews[0::2]\n",
    "\n",
    "    dataframe = pd.DataFrame({'Names':names,'Short_Reviews':short_reviews,'Reviews':long_reviews,'Date':dates})\n",
    "    \n",
    "    dataframe1 = dataframe1.append(dataframe,ignore_index=True)\n",
    "\n",
    "    i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This block of code will help extract the short reviews of the product\n",
    "\n",
    "names = []\n",
    "shor_reviews=[]\n",
    "usefullness = []\n",
    "dates = []\n",
    "\n",
    "for span in soup.findAll('span',attrs={'class': 'a-profile-name'}):\n",
    "    name = span.text.strip()\n",
    "    names.append(name)\n",
    "names =names [2::1]\n",
    "\n",
    "for span in soup.findAll('span',attrs={'class': 'cr-original-review-content'}):\n",
    "    short_review = span.text.strip()\n",
    "    shor_reviews.append(short_review)\n",
    "    \n",
    "for span in soup.findAll('span',attrs={'class': 'a-size-base a-color-tertiary cr-vote-text'}):\n",
    "    use = span.text.strip()\n",
    "    usefullness.append(use)\n",
    "    \n",
    "for span in soup.findAll('span',attrs={'class': 'a-size-base a-color-secondary review-date'}):\n",
    "    date = span.text.strip()\n",
    "    dates.append(date)\n",
    "\n",
    "dates =dates [2::1]\n",
    "\n",
    "long_reviews = shor_reviews[1::2]\n",
    "short_reviews = shor_reviews[0::2]\n",
    "\n",
    "dataframe = pd.DataFrame({'Names':names,'Short_Reviews':short_reviews,'Reviews':long_reviews,'Date':dates,'Usefullness':usefullness\n",
    "                })\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "i=2\n",
    "html = urlopen(\"https://www.amazon.in/All-new-Echo-Dot-3rd-Gen/product-reviews/B0792KTHKK/ref=cm_cr_getr_d_paging_btm_next_{}?showViewpoints=1&pageNumber={}&reviewerType=all_reviews\".format(i,i))\n",
    "soup = BeautifulSoup(html,\"lxml\")\n",
    "html = soup.prettify('utf-8')\n",
    "    \n",
    "names = []\n",
    "shor_reviews=[]\n",
    "ratings = []\n",
    "dates = []\n",
    "\n",
    "for span in soup.findAll('span',attrs={'class': 'a-profile-name'}):\n",
    "    name = span.text.strip()\n",
    "    names.append(name)\n",
    "\n",
    "names =names [2::1]\n",
    "\n",
    "for span in soup.findAll('span',attrs={'class': 'cr-original-review-content'}):\n",
    "    short_review = span.text.strip()\n",
    "    shor_reviews.append(short_review)\n",
    "    \n",
    "for span in soup.findAll('span',attrs={'class': 'a-icon-alt'}):\n",
    "    rate = span.text.strip()\n",
    "    ratings.append(rate)\n",
    "ratings = ratings[3:11:1]\n",
    "    \n",
    "for span in soup.findAll('span',attrs={'class': 'a-size-base a-color-secondary review-date'}):\n",
    "    date = span.text.strip()\n",
    "    dates.append(date)\n",
    "\n",
    "dates =dates [2::1]\n",
    "\n",
    "long_reviews = shor_reviews[1::2]\n",
    "short_reviews = shor_reviews[0::2]\n",
    "\n",
    "#dataframe = pd.DataFrame({'Names':names,'Short_Reviews':short_reviews,'Reviews':long_reviews,'Date':dates,'Usefullness':usefullness\n",
    "              # })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Remove Punctuations\n",
    "\n",
    "string.punctuation\n",
    "def remove_punct(text):\n",
    "    text_nonpunct = \"\".join([char for char in text if char not in string.punctuation])\n",
    "    return text_nonpunct\n",
    "\n",
    "alexa['Clean_Review'] = alexa['Review'].apply(lambda x: remove_punct(x))\n",
    "alexa.head()\n",
    "\n",
    "# Tokenize \n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = re.split('\\W+',text)\n",
    "    return tokens\n",
    "\n",
    "alexa['Clean_Review_tok'] = alexa['Clean_Review'].apply(lambda x: tokenize(x.lower()))\n",
    "alexa.head()\n",
    "\n",
    "\n",
    "\n",
    "def remove_stop(tok_list):\n",
    "    text = [word for word in tok_list if word not in stopword]\n",
    "    return text\n",
    "\n",
    "alexa['Clean_Review_Stop'] = alexa['Clean_Review_tok'].apply(lambda x: remove_stop(x))\n",
    "alexa.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataframe = pd.DataFrame({'Names':names,'Short_Reviews':short_reviews,'Reviews':long_reviews,'Date':dates,'Ratings':ratings\n",
    "              })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(448,500):\n",
    "    print(i)\n",
    "    html = urlopen(\"https://www.amazon.in/All-new-Echo-Dot-3rd-Gen/product-reviews/B0792KTHKK/ref=cm_cr_getr_d_paging_btm_next_{}?showViewpoints=1&pageNumber={}&reviewerType=all_reviews\".format(i,i))\n",
    "    #US :html = urlopen(\"https://www.amazon.com/Echo-Dot-3rd-Gen-Sandstone/product-reviews/B0792R1RSN/ref=cm_cr_arp_d_paging_btm_next_{}?ie=UTF8&reviewerType=all_reviews&pageNumber={}\".format(i,i))\n",
    "    #html = urlopen(\"https://www.amazon.ca/All-new-Echo-Dot-3rd-gen/product-reviews/B0792JYXZK/ref=cm_cr_arp_d_paging_btm_next_{}?showViewpoints=1&pageNumber={}\".format(i,i))\n",
    "    soup = BeautifulSoup(html,\"lxml\")\n",
    "    html = soup.prettify('utf-8')\n",
    "    \n",
    "    names = []\n",
    "    shor_reviews=[]\n",
    "    #usefullness = []\n",
    "    dates = []\n",
    "\n",
    "    for span in soup.findAll('span',attrs={'class': 'a-profile-name'}):\n",
    "        name = span.text.strip()\n",
    "        names.append(name)\n",
    "    names =names [2::1]\n",
    "\n",
    "    for span in soup.findAll('span',attrs={'class': 'cr-original-review-content'}):\n",
    "        short_review = span.text.strip()\n",
    "        shor_reviews.append(short_review)\n",
    "    \n",
    "    #for span in soup.findAll('span',attrs={'class': 'a-size-base a-color-tertiary cr-vote-text'}):\n",
    "    #    use = span.text.strip()\n",
    "    #    usefullness.append(use)\n",
    "    \n",
    "    \n",
    "    for span in soup.findAll('span',attrs={'class': 'a-size-base a-color-secondary review-date'}):\n",
    "        date = span.text.strip()\n",
    "        dates.append(date)\n",
    "\n",
    "    dates =dates [2::1]\n",
    "\n",
    "    long_reviews = shor_reviews[1::2]\n",
    "    short_reviews = shor_reviews[0::2]\n",
    "\n",
    "    dataframe = pd.DataFrame({'Names':names,'Short_Reviews':short_reviews,'Reviews':long_reviews,'Date':dates})\n",
    "    \n",
    "    dataframe1 = dataframe1.append(dataframe,ignore_index=True)\n",
    "\n",
    "    i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
