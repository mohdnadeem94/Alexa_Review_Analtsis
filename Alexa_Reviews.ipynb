{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from statistics import mean \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Web scarapping \n",
    "\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "#Error Handling \n",
    "import sys\n",
    "import traceback\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "os.chdir('C:/Users/mohdn/OneDrive/Desktop/Nadeem/DataSets/Alexa')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import urllib.parse\n",
    "import urllib.error\n",
    "from bs4 import BeautifulSoup\n",
    "import ssl\n",
    "import json\n",
    "\n",
    "ctx = ssl.create_default_context()\n",
    "ctx.check_hostname = False\n",
    "ctx.verify_mode = ssl.CERT_NONE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe1=pd.DataFrame()\n",
    "print(dataframe1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#India:\n",
    "j=2\n",
    "e='Exception message : HTTP Error 503: Service Unavailable'\n",
    "while(j<370 and e==('Exception message : HTTP Error 503: Service Unavailable')):\n",
    "    try:\n",
    "        for i in range(j,370):\n",
    "            print(i)\n",
    "            html = urlopen(\"https://www.amazon.in/All-new-Echo-Dot-3rd-Gen/product-reviews/B0792KTHKK/ref=cm_cr_getr_d_paging_btm_next_{}?showViewpoints=1&pageNumber={}&reviewerType=all_reviews\".format(i,i))\n",
    "            soup = BeautifulSoup(html,\"lxml\")\n",
    "            html = soup.prettify('utf-8')\n",
    "\n",
    "            names = []\n",
    "            shor_reviews=[]\n",
    "            #usefullness = []\n",
    "            dates = []\n",
    "            color = []\n",
    "\n",
    "            for span in soup.findAll('span',attrs={'class': 'a-profile-name'}):\n",
    "                name = span.text.strip()\n",
    "                names.append(name)\n",
    "            names =names [2::1]\n",
    "\n",
    "            for span in soup.findAll('span',attrs={'class': ''}):\n",
    "                short_review = span.text.strip()\n",
    "                shor_reviews.append(short_review)\n",
    "\n",
    "        #for span in soup.findAll('span',attrs={'class': 'a-size-base a-color-tertiary cr-vote-text'}):\n",
    "        #    use = span.text.strip()\n",
    "        #    usefullness.append(use)\n",
    "            for span in soup.findAll('a',attrs={'class': \"a-size-mini a-link-normal a-color-secondary\",'data-hook':\"format-strip\"}):\n",
    "                b = span.text.strip()\n",
    "                if b[8:10] == 'Bl':\n",
    "                    b = b[8:13]\n",
    "                if b[8:10] == 'Wh':\n",
    "                    b = b[8:13]\n",
    "                if b[8:10] == 'Gr':\n",
    "                    b = b[8:12]\n",
    "                color.append(b)\n",
    "\n",
    "\n",
    "            for span in soup.findAll('span',attrs={'class': 'a-size-base a-color-secondary review-date'}):\n",
    "                date = span.text.strip()\n",
    "                dates.append(date)\n",
    "\n",
    "            dates =dates [2::1]\n",
    "\n",
    "            long_reviews = shor_reviews[2::4]\n",
    "            short_reviews = shor_reviews[1::4]\n",
    "            short_reviews=short_reviews[:-1]\n",
    "            long_reviews=long_reviews[:-1]\n",
    "\n",
    "            dataframe = pd.DataFrame({'Names':names,'Short_Reviews':short_reviews,'Reviews':long_reviews,'Date':dates,'Color':color})\n",
    "    \n",
    "\n",
    "            dataframe1 = dataframe1.append(dataframe,ignore_index=True)\n",
    "\n",
    "            i=i+1\n",
    "\n",
    "    except BaseException as ex:\n",
    "        # Get current system exception\n",
    "        ex_type, ex_value, ex_traceback = sys.exc_info()\n",
    "\n",
    "        # Extract unformatter stack traces as tuples\n",
    "        trace_back = traceback.extract_tb(ex_traceback)\n",
    "\n",
    "        # Format stacktrace\n",
    "        stack_trace = list()\n",
    "\n",
    "        for trace in trace_back:\n",
    "            stack_trace.append(\"File : %s , Line : %d, Func.Name : %s, Message : %s\" % (trace[0], trace[1], trace[2], trace[3]))\n",
    "\n",
    "        #print(\"Exception type : %s \" % ex_type.__name__)\n",
    "        #print(\"Exception message : %s\" %ex_value)\n",
    "        #print(\"Stack trace : %s\" %stack_trace)\n",
    "        e = (\"Exception message : %s\" %ex_value)\n",
    "    j=i\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##UK Spl:\n",
    "\n",
    "j=237\n",
    "e='Exception message : HTTP Error 503: Service Unavailable'\n",
    "while(j<501 and e==('Exception message : HTTP Error 503: Service Unavailable')):\n",
    "    try:\n",
    "        for i in range(j,501):\n",
    "            print(i)\n",
    "            html = urlopen(\"https://www.amazon.co.uk/All-new-Echo-Dot-3rd-Gen/product-reviews/B0792KWK57/ref=cm_cr_arp_d_paging_btm_next_{}?ie=UTF8&reviewerType=all_reviews&pageNumber={}\".format(i,i))\n",
    "            soup = BeautifulSoup(html,\"lxml\")\n",
    "            html = soup.prettify('utf-8')\n",
    "\n",
    "            names = []\n",
    "            shor_reviews=[]\n",
    "            #usefullness = []\n",
    "            dates = []\n",
    "            color = []\n",
    "\n",
    "            for span in soup.findAll('span',attrs={'class': 'a-profile-name'}):\n",
    "                name = span.text.strip()\n",
    "                names.append(name)\n",
    "            names =names [2::1]\n",
    "\n",
    "            for span in soup.findAll('span',attrs={'class': ''}):\n",
    "                short_review = span.text.strip()\n",
    "                shor_reviews.append(short_review)\n",
    "\n",
    "        #for span in soup.findAll('span',attrs={'class': 'a-size-base a-color-tertiary cr-vote-text'}):\n",
    "        #    use = span.text.strip()\n",
    "        #    usefullness.append(use)\n",
    "            for span in soup.findAll('a',attrs={'class': \"a-size-mini a-link-normal a-color-secondary\",'data-hook':\"format-strip\"}):\n",
    "                b = span.text.strip()\n",
    "                if b[8:10] == 'Ch':\n",
    "                    b = b[8:16]\n",
    "                if b[8:10] == 'He':\n",
    "                    b = b[8:20]\n",
    "                if b[8:10] == 'Sa':\n",
    "                    b = b[8:17]\n",
    "                color.append(b)\n",
    "\n",
    "\n",
    "            for span in soup.findAll('span',attrs={'class': 'a-size-base a-color-secondary review-date'}):\n",
    "                date = span.text.strip()\n",
    "                dates.append(date)\n",
    "\n",
    "            dates =dates [2::1]\n",
    "\n",
    "            long_reviews = shor_reviews[1::4]\n",
    "            short_reviews = shor_reviews[0::4]\n",
    "            short_reviews=short_reviews[:-1]\n",
    "\n",
    "\n",
    "            dataframe = pd.DataFrame({'Names':names,'Short_Reviews':short_reviews,'Reviews':long_reviews,'Date':dates,'Color':color})\n",
    "\n",
    "            dataframe1 = dataframe1.append(dataframe,ignore_index=True)\n",
    "\n",
    "            i=i+1\n",
    "\n",
    "    except BaseException as ex:\n",
    "        # Get current system exception\n",
    "        ex_type, ex_value, ex_traceback = sys.exc_info()\n",
    "\n",
    "        # Extract unformatter stack traces as tuples\n",
    "        trace_back = traceback.extract_tb(ex_traceback)\n",
    "\n",
    "        # Format stacktrace\n",
    "        stack_trace = list()\n",
    "\n",
    "        for trace in trace_back:\n",
    "            stack_trace.append(\"File : %s , Line : %d, Func.Name : %s, Message : %s\" % (trace[0], trace[1], trace[2], trace[3]))\n",
    "\n",
    "        #print(\"Exception type : %s \" % ex_type.__name__)\n",
    "        #print(\"Exception message : %s\" %ex_value)\n",
    "        #print(\"Stack trace : %s\" %stack_trace)\n",
    "        e = (\"Exception message : %s\" %ex_value)\n",
    "    j=i\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Canda:\n",
    "\n",
    "j=2\n",
    "e='Exception message : HTTP Error 503: Service Unavailable'\n",
    "while(j<304 and e==('Exception message : HTTP Error 503: Service Unavailable')):\n",
    "    try:\n",
    "        for i in range(j,304):\n",
    "            print(i)\n",
    "            html = urlopen(\"https://www.amazon.ca/All-new-Echo-Dot-3rd-gen/product-reviews/B0792JYXZK/ref=cm_cr_arp_d_paging_btm_next_{}?showViewpoints=1&pageNumber={}\".format(i,i))\n",
    "            soup = BeautifulSoup(html,\"lxml\")\n",
    "            html = soup.prettify('utf-8')\n",
    "\n",
    "            names = []\n",
    "            shor_reviews=[]\n",
    "            #usefullness = []\n",
    "            dates = []\n",
    "            color =[]\n",
    "\n",
    "            for span in soup.findAll('span',attrs={'class': 'a-profile-name'}):\n",
    "                name = span.text.strip()\n",
    "                names.append(name)\n",
    "            names =names [2::1]\n",
    "\n",
    "            for span in soup.findAll('span',attrs={'class': ''}):\n",
    "                short_review = span.text.strip()\n",
    "                shor_reviews.append(short_review)\n",
    "                \n",
    "            for span in soup.findAll('a',attrs={'class': \"a-size-mini a-link-normal a-color-secondary\",'data-hook':\"format-strip\"}):\n",
    "                b = span.text.strip()\n",
    "                if b[8:10] == 'Ch':\n",
    "                    b = b[8:16]\n",
    "                if b[8:10] == 'He':\n",
    "                    b = b[8:20]\n",
    "                if b[8:10] == 'Sa':\n",
    "                    b = b[8:17]\n",
    "                color.append(b)\n",
    "\n",
    "        #for span in soup.findAll('span',attrs={'class': 'a-size-base a-color-tertiary cr-vote-text'}):\n",
    "        #    use = span.text.strip()\n",
    "        #    usefullness.append(use)\n",
    "\n",
    "\n",
    "            for span in soup.findAll('span',attrs={'class': 'a-size-base a-color-secondary review-date'}):\n",
    "                date = span.text.strip()\n",
    "                dates.append(date)\n",
    "\n",
    "            dates =dates [2::1]\n",
    "\n",
    "            long_reviews = shor_reviews[2::4]\n",
    "            short_reviews = shor_reviews[1::4]\n",
    "            short_reviews=short_reviews[:-1]\n",
    "            long_reviews=long_reviews[:-1]\n",
    "\n",
    "            dataframe = pd.DataFrame({'Names':names,'Short_Reviews':short_reviews,'Reviews':long_reviews,'Date':dates,'Color':color})\n",
    "\n",
    "            dataframe1 = dataframe1.append(dataframe,ignore_index=True)\n",
    "\n",
    "            i=i+1\n",
    "\n",
    "    except BaseException as ex:\n",
    "        # Get current system exception\n",
    "        ex_type, ex_value, ex_traceback = sys.exc_info()\n",
    "\n",
    "        # Extract unformatter stack traces as tuples\n",
    "        trace_back = traceback.extract_tb(ex_traceback)\n",
    "\n",
    "        # Format stacktrace\n",
    "        stack_trace = list()\n",
    "\n",
    "        for trace in trace_back:\n",
    "            stack_trace.append(\"File : %s , Line : %d, Func.Name : %s, Message : %s\" % (trace[0], trace[1], trace[2], trace[3]))\n",
    "\n",
    "        #print(\"Exception type : %s \" % ex_type.__name__)\n",
    "        #print(\"Exception message : %s\" %ex_value)\n",
    "        #print(\"Stack trace : %s\" %stack_trace)\n",
    "        e = (\"Exception message : %s\" %ex_value)\n",
    "    j=i\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#US:\n",
    "\n",
    "j=501\n",
    "e='Exception message : HTTP Error 503: Service Unavailable'\n",
    "while(j<2500 and e==('Exception message : HTTP Error 503: Service Unavailable')):\n",
    "    try:\n",
    "        for i in range(j,502):\n",
    "            print(i)\n",
    "            html = urlopen(\"https://www.amazon.com/Echo-Dot-3rd-Gen-improved/product-reviews/B0792KTHKJ/ref=cm_cr_arp_d_paging_btm_next_{}?ie=UTF8&reviewerType=all_reviews&pageNumber={}\".format(i,i))\n",
    "            soup = BeautifulSoup(html,\"lxml\")\n",
    "            html = soup.prettify('utf-8')\n",
    "\n",
    "            names = []\n",
    "            shor_reviews=[]\n",
    "            #usefullness = []\n",
    "            dates = []\n",
    "            color =[]\n",
    "\n",
    "            for span in soup.findAll('span',attrs={'class': 'a-profile-name'}):\n",
    "                name = span.text.strip()\n",
    "                names.append(name)\n",
    "            names =names [2::1]\n",
    "\n",
    "            for span in soup.findAll('span',attrs={'class': ''}):\n",
    "                short_review = span.text.strip()\n",
    "                shor_reviews.append(short_review)\n",
    "                \n",
    "            for span in soup.findAll('a',attrs={'class': \"a-size-mini a-link-normal a-color-secondary\",'data-hook':\"format-strip\"}):\n",
    "                b = span.text.strip()\n",
    "                if b[7:9] == 'Ch':\n",
    "                    b = b[7:15]\n",
    "                if b[7:9] == 'He':\n",
    "                    b = b[7:19]\n",
    "                if b[7:9] == 'Sa':\n",
    "                    b = b[7:16]\n",
    "                color.append(b)\n",
    "\n",
    "        #for span in soup.findAll('span',attrs={'class': 'a-size-base a-color-tertiary cr-vote-text'}):\n",
    "        #    use = span.text.strip()\n",
    "        #    usefullness.append(use)\n",
    "\n",
    "\n",
    "            for span in soup.findAll('span',attrs={'class': 'a-size-base a-color-secondary review-date'}):\n",
    "                date = span.text.strip()\n",
    "                dates.append(date)\n",
    "\n",
    "            dates =dates [2::1]\n",
    "\n",
    "            long_reviews = shor_reviews[1::4]\n",
    "            short_reviews = shor_reviews[0::4]\n",
    "            short_reviews=short_reviews[:-1]\n",
    "            long_reviews=long_reviews[:-1]\n",
    "\n",
    "            dataframe = pd.DataFrame({'Names':names,'Short_Reviews':short_reviews,'Reviews':long_reviews,'Date':dates,'Color':color})\n",
    "\n",
    "            dataframe1 = dataframe1.append(dataframe,ignore_index=True)\n",
    "\n",
    "            i=i+1\n",
    "\n",
    "    except BaseException as ex:\n",
    "        # Get current system exception\n",
    "        ex_type, ex_value, ex_traceback = sys.exc_info()\n",
    "\n",
    "        # Extract unformatter stack traces as tuples\n",
    "        trace_back = traceback.extract_tb(ex_traceback)\n",
    "\n",
    "        # Format stacktrace\n",
    "        stack_trace = list()\n",
    "\n",
    "        for trace in trace_back:\n",
    "            stack_trace.append(\"File : %s , Line : %d, Func.Name : %s, Message : %s\" % (trace[0], trace[1], trace[2], trace[3]))\n",
    "\n",
    "        #print(\"Exception type : %s \" % ex_type.__name__)\n",
    "        #print(\"Exception message : %s\" %ex_value)\n",
    "        #print(\"Stack trace : %s\" %stack_trace)\n",
    "        e = (\"Exception message : %s\" %ex_value)\n",
    "    j=i\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Australia:\n",
    "\n",
    "j=1\n",
    "e='Exception message : HTTP Error 503: Service Unavailable'\n",
    "while(j<12 and e==('Exception message : HTTP Error 503: Service Unavailable')):\n",
    "    try:\n",
    "        for i in range(j,12):\n",
    "            print(i)\n",
    "            #html = urlopen(\"https://www.amazon.com/Echo-Dot-3rd-Gen-improved/product-reviews/B0792KTHKJ/ref=cm_cr_arp_d_paging_btm_next_{}?ie=UTF8&reviewerType=all_reviews&pageNumber={}\".format(i,i))\n",
    "            html = urlopen(\"https://www.amazon.com.au/All-new-Echo-Smart-speaker-Alexa/product-reviews/B0792KRW2J/ref=cm_cr_getr_d_paging_btm_next_{}?showViewpoints=1&pageNumber={}\".format(i,i))\n",
    "            soup = BeautifulSoup(html,\"lxml\")\n",
    "            html = soup.prettify('utf-8')\n",
    "\n",
    "            names = []\n",
    "            shor_reviews=[]\n",
    "            #usefullness = []\n",
    "            dates = []\n",
    "            color =[]\n",
    "\n",
    "            for span in soup.findAll('span',attrs={'class': 'a-profile-name'}):\n",
    "                name = span.text.strip()\n",
    "                names.append(name)\n",
    "            names =names [2::1]\n",
    "\n",
    "            for span in soup.findAll('span',attrs={'class': ''}):\n",
    "                short_review = span.text.strip()\n",
    "                shor_reviews.append(short_review)\n",
    "                \n",
    "            for span in soup.findAll('a',attrs={'class': \"a-size-mini a-link-normal a-color-secondary\",'data-hook':\"format-strip\"}):\n",
    "                b = span.text.strip()\n",
    "                if b[7:9] == 'Ch':\n",
    "                    b = b[7:15]\n",
    "                if b[7:9] == 'He':\n",
    "                    b = b[7:19]\n",
    "                if b[7:9] == 'Sa':\n",
    "                    b = b[7:16]\n",
    "                color.append(b)\n",
    "\n",
    "        #for span in soup.findAll('span',attrs={'class': 'a-size-base a-color-tertiary cr-vote-text'}):\n",
    "        #    use = span.text.strip()\n",
    "        #    usefullness.append(use)\n",
    "\n",
    "\n",
    "            for span in soup.findAll('span',attrs={'class': 'a-size-base a-color-secondary review-date'}):\n",
    "                date = span.text.strip()\n",
    "                dates.append(date)\n",
    "\n",
    "            dates =dates [2::1]\n",
    "\n",
    "\n",
    "            long_reviews = shor_reviews[2::4]\n",
    "            short_reviews = shor_reviews[1::4]\n",
    "            short_reviews=short_reviews[:-1]\n",
    "            long_reviews=long_reviews[:-1]\n",
    "\n",
    "            dataframe = pd.DataFrame({'Names':names,'Short_Reviews':short_reviews,'Reviews':long_reviews,'Date':dates,'Color':color})\n",
    "\n",
    "            dataframe1 = dataframe1.append(dataframe,ignore_index=True)\n",
    "\n",
    "            i=i+1\n",
    "\n",
    "    except BaseException as ex:\n",
    "        # Get current system exception\n",
    "        ex_type, ex_value, ex_traceback = sys.exc_info()\n",
    "\n",
    "        # Extract unformatter stack traces as tuples\n",
    "        trace_back = traceback.extract_tb(ex_traceback)\n",
    "\n",
    "        # Format stacktrace\n",
    "        stack_trace = list()\n",
    "\n",
    "        for trace in trace_back:\n",
    "            stack_trace.append(\"File : %s , Line : %d, Func.Name : %s, Message : %s\" % (trace[0], trace[1], trace[2], trace[3]))\n",
    "\n",
    "        #print(\"Exception type : %s \" % ex_type.__name__)\n",
    "        #print(\"Exception message : %s\" %ex_value)\n",
    "        #print(\"Stack trace : %s\" %stack_trace)\n",
    "        e = (\"Exception message : %s\" %ex_value)\n",
    "    j=i\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Exporting CSV files\n",
    "#export_csv_india = dataframe1.to_csv(r'C:\\Users\\mohdn\\OneDrive\\Desktop\\Nadeem\\DataSets\\Alexa\\Alexa_Reviews_India.csv',index=False)\n",
    "#export_csv_uk = dataframe1.to_csv(r'C:\\Users\\mohdn\\OneDrive\\Desktop\\Nadeem\\DataSets\\Alexa\\Alexa_Reviews_UK.csv',index=False)\n",
    "#export_csv_canada = dataframe1.to_csv(r'C:\\Users\\mohdn\\OneDrive\\Desktop\\Nadeem\\DataSets\\Alexa\\Alexa_Reviews_Canada.csv',index=False)\n",
    "#export_csv_usa = dataframe1.to_csv(r'C:\\Users\\mohdn\\OneDrive\\Desktop\\Nadeem\\DataSets\\Alexa\\Alexa_Reviews_USA.csv',index=False)\n",
    "export_csv_aus = dataframe1.to_csv(r'C:\\Users\\mohdn\\OneDrive\\Desktop\\Nadeem\\DataSets\\Alexa\\Alexa_Reviews_Australia.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading the files\n",
    "\n",
    "country_list = ['India','UK','Canada','USA','Australia']\n",
    "i=0\n",
    "alexa = pd.DataFrame()\n",
    "for country in country_list:\n",
    "    individual = pd.read_csv('Alexa_Reviews_{}.csv'.format(country),encoding='latin-1')\n",
    "    j=country_list[i]\n",
    "    print(j)\n",
    "    i=i+1\n",
    "    individual['Country'] = str(j)\n",
    "    alexa = alexa.append(individual,ignore_index=True)\n",
    "    \n",
    "    \n",
    "stopword = nltk.corpus.stopwords.words('english')\n",
    "ps = nltk.PorterStemmer()\n",
    "wn=nltk.WordNetLemmatizer()\n",
    "sid = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Analysing the product:\n",
    "\n",
    "alexa['Color'] = alexa['Color'].replace(['Heather Gray','Grey'],'Heather Grey')\n",
    "alexa['Color'] = alexa['Color'].replace('Black','Charcoal')\n",
    "\n",
    "uni_colors = np.unique(alexa['Color'])\n",
    "\n",
    "for uni in uni_colors:\n",
    "    print('Count of total {} color ='.format(uni),alexa['Color'][alexa['Color']==uni].count())\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "uni_country = np.unique(alexa['Country'])\n",
    "for country in uni_country:\n",
    "    uni_colors_country = np.unique(alexa['Color'][alexa['Country']==country])\n",
    "    country1 = []\n",
    "    for uni in uni_colors_country:\n",
    "        #print('Count of {} color in {} ='.format(uni,country),alexa['Color'][(alexa.Color==uni )& (alexa.Country ==country)].count())\n",
    "        country_count = alexa['Color'][(alexa.Color==uni )& (alexa.Country ==country)].count()\n",
    "        country1.append(country_count)\n",
    "    new = pd.DataFrame(country1,uni_colors_country)\n",
    "    new =new.reset_index()\n",
    "    new.columns = ['Color','Count']\n",
    "    print(new)\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#obtaining the sentiment scores for the reviews.\n",
    "\n",
    "scores = []\n",
    "for review in (alexa['Reviews']):\n",
    "    score = sid.polarity_scores(str(review))\n",
    "    scores.append(score)\n",
    "scores = pd.DataFrame.from_dict(scores)\n",
    "\n",
    "country_wise = alexa[['Country','Reviews']]\n",
    "country_wise = pd.concat([country_wise,scores],axis=1)\n",
    "\n",
    "country_wise.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Overall mean\n",
    "\n",
    "comp_mean = country_wise['compound'].mean()\n",
    "neg_mean = country_wise['neg'].mean()\n",
    "neu_mean = country_wise['neu'].mean()\n",
    "pos_mean = country_wise['pos'].mean()\n",
    "\n",
    "list_val = [comp_mean,neg_mean,neu_mean,pos_mean]\n",
    "list_names = ['comp_mean','neg_mean','neu_mean','pos_mean']\n",
    "\n",
    "list_df = pd.DataFrame({'name':list_names,\n",
    "                        'values':list_val})\n",
    "\n",
    "print(plt.bar(list_names,list_val, align='center', alpha=0.5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Country wise Mean\n",
    "\n",
    "country = [\"Canada\",\"Australia\",\"UK\"]\n",
    "rating=[]\n",
    "compound=[]\n",
    "neg=[]\n",
    "neu=[]\n",
    "pos=[]\n",
    "\n",
    "for item in country:\n",
    "    \n",
    "    \n",
    "    subset = country_wise[country_wise['Country']==item]\n",
    "    \n",
    "    c_comp ='Mean_Compound_{}'.format(item)\n",
    "    avg_comp= str(subset['compound'].mean())\n",
    "    print('{} is {}'.format(c_comp,avg_comp))\n",
    "    compound.append(avg_comp)\n",
    "    \n",
    "    c_neg ='Mean_Negativity_{}'.format(item)\n",
    "    avg_neg= str(subset['neg'].mean())\n",
    "    print('{} is {}'.format(c_neg,avg_neg))\n",
    "    neg.append(avg_neg)\n",
    "    \n",
    "    c_neu ='Mean_Neutral_{}'.format(item)\n",
    "    avg_neu= str(subset['neu'].mean())\n",
    "    print('{} is {}'.format(c_neu,avg_neu))\n",
    "    neu.append(avg_neu)\n",
    "    \n",
    "    c_pos ='Mean_Postivity_{}'.format(item)\n",
    "    avg_pos= str(subset['pos'].mean())\n",
    "    print('{} is {}'.format(c_pos,avg_pos))\n",
    "    pos.append(avg_pos)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "new_list = [compound,pos,neg,neu]\n",
    "i=1\n",
    "for item in new_list:\n",
    "    plt.subplot(4,1,i)\n",
    "    plt.bar(country,item, align='center', alpha=0.5)\n",
    "    i=i+1\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text =\"\".join([char for char in text if char not in string.punctuation])\n",
    "    token = re.split('\\W+',text)\n",
    "    stop_word = [word for word in token if word not in stopword]\n",
    "    #text = [ps.stem(word) for word in stop_word]\n",
    "    text = [wn.lemmatize(word) for word in stop_word]\n",
    "    return text\n",
    "\n",
    "alexa['Clean_Review'] = alexa['Review'].apply(lambda x: clean_text(x.lower()))\n",
    "#alexa['Short_Clean_Review'] = alexa['Short Review'].apply(lambda x: clean_text(x.lower()))\n",
    "alexa.head()  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DEBUGGING AREA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This block of code will help extract the short reviews of the product\n",
    "\n",
    "names = []\n",
    "shor_reviews=[]\n",
    "usefullness = []\n",
    "dates = []\n",
    "\n",
    "for span in soup.findAll('span',attrs={'class': 'a-profile-name'}):\n",
    "    name = span.text.strip()\n",
    "    names.append(name)\n",
    "names =names [2::1]\n",
    "\n",
    "for span in soup.findAll('span',attrs={'class': 'cr-original-review-content'}):\n",
    "    short_review = span.text.strip()\n",
    "    shor_reviews.append(short_review)\n",
    "    \n",
    "for span in soup.findAll('span',attrs={'class': 'a-size-base a-color-tertiary cr-vote-text'}):\n",
    "    use = span.text.strip()\n",
    "    usefullness.append(use)\n",
    "    \n",
    "for span in soup.findAll('span',attrs={'class': 'a-size-base a-color-secondary review-date'}):\n",
    "    date = span.text.strip()\n",
    "    dates.append(date)\n",
    "\n",
    "dates =dates [2::1]\n",
    "\n",
    "long_reviews = shor_reviews[1::2]\n",
    "short_reviews = shor_reviews[0::2]\n",
    "\n",
    "dataframe = pd.DataFrame({'Names':names,'Short_Reviews':short_reviews,'Reviews':long_reviews,'Date':dates,'Usefullness':usefullness\n",
    "                })\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "i=2\n",
    "html = urlopen(\"https://www.amazon.in/All-new-Echo-Dot-3rd-Gen/product-reviews/B0792KTHKK/ref=cm_cr_getr_d_paging_btm_next_{}?showViewpoints=1&pageNumber={}&reviewerType=all_reviews\".format(i,i))\n",
    "soup = BeautifulSoup(html,\"lxml\")\n",
    "html = soup.prettify('utf-8')\n",
    "    \n",
    "names = []\n",
    "shor_reviews=[]\n",
    "ratings = []\n",
    "dates = []\n",
    "\n",
    "for span in soup.findAll('span',attrs={'class': 'a-profile-name'}):\n",
    "    name = span.text.strip()\n",
    "    names.append(name)\n",
    "\n",
    "names =names [2::1]\n",
    "\n",
    "for span in soup.findAll('span',attrs={'class': 'cr-original-review-content'}):\n",
    "    short_review = span.text.strip()\n",
    "    shor_reviews.append(short_review)\n",
    "    \n",
    "for span in soup.findAll('span',attrs={'class': 'a-icon-alt'}):\n",
    "    rate = span.text.strip()\n",
    "    ratings.append(rate)\n",
    "ratings = ratings[3:11:1]\n",
    "    \n",
    "for span in soup.findAll('span',attrs={'class': 'a-size-base a-color-secondary review-date'}):\n",
    "    date = span.text.strip()\n",
    "    dates.append(date)\n",
    "\n",
    "dates =dates [2::1]\n",
    "\n",
    "long_reviews = shor_reviews[1::2]\n",
    "short_reviews = shor_reviews[0::2]\n",
    "\n",
    "#dataframe = pd.DataFrame({'Names':names,'Short_Reviews':short_reviews,'Reviews':long_reviews,'Date':dates,'Usefullness':usefullness\n",
    "              # })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Remove Punctuations\n",
    "\n",
    "string.punctuation\n",
    "def remove_punct(text):\n",
    "    text_nonpunct = \"\".join([char for char in text if char not in string.punctuation])\n",
    "    return text_nonpunct\n",
    "\n",
    "alexa['Clean_Review'] = alexa['Review'].apply(lambda x: remove_punct(x))\n",
    "alexa.head()\n",
    "\n",
    "# Tokenize \n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = re.split('\\W+',text)\n",
    "    return tokens\n",
    "\n",
    "alexa['Clean_Review_tok'] = alexa['Clean_Review'].apply(lambda x: tokenize(x.lower()))\n",
    "alexa.head()\n",
    "\n",
    "\n",
    "\n",
    "def remove_stop(tok_list):\n",
    "    text = [word for word in tok_list if word not in stopword]\n",
    "    return text\n",
    "\n",
    "alexa['Clean_Review_Stop'] = alexa['Clean_Review_tok'].apply(lambda x: remove_stop(x))\n",
    "alexa.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataframe = pd.DataFrame({'Names':names,'Short_Reviews':short_reviews,'Reviews':long_reviews,'Date':dates,'Ratings':ratings\n",
    "              })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(2,3):\n",
    "    print(i)\n",
    "     \n",
    "    html = urlopen(\"https://www.amazon.com.au/All-new-Echo-Smart-speaker-Alexa/product-reviews/B0792KRW2J/ref=cm_cr_getr_d_paging_btm_next_1?showViewpoints=1&pageNumber=1\")\n",
    "    soup = BeautifulSoup(html,\"lxml\")\n",
    "    html = soup.prettify('utf-8')\n",
    "    names = []\n",
    "    shor_reviews=[]\n",
    "    #usefullness = []\n",
    "    dates = []\n",
    "    color=[]\n",
    "\n",
    "    for span in soup.findAll('span',attrs={'class': 'a-profile-name'}):\n",
    "        name = span.text.strip()\n",
    "        names.append(name)\n",
    "    names =names [2::1]\n",
    "\n",
    "    for span in soup.findAll('span',attrs={'class': ''}):\n",
    "        short_review = span.text.strip()\n",
    "        shor_reviews.append(short_review)\n",
    "    \n",
    "    #for span in soup.findAll('span',attrs={'class': 'a-size-base a-color-tertiary cr-vote-text'}):\n",
    "    #    use = span.text.strip()\n",
    "    #    usefullness.append(use)\n",
    "    \n",
    "    \n",
    "    for span in soup.findAll('a',attrs={'class': \"a-size-mini a-link-normal a-color-secondary\",'data-hook':\"format-strip\"}):\n",
    "        b = span.text.strip()\n",
    "        if b[7:9] == 'Ch':\n",
    "            b = b[7:15]\n",
    "        if b[7:9] == 'He':\n",
    "            b = b[7:19]\n",
    "        if b[7:9] == 'Sa':\n",
    "            b = b[7:16]\n",
    "        color.append(b)\n",
    "        \n",
    "    for span in soup.findAll('span',attrs={'class': 'a-size-base a-color-secondary review-date'}):\n",
    "        date = span.text.strip()\n",
    "        dates.append(date)\n",
    "        \n",
    "        \n",
    "\n",
    "    dates =dates [2::1]\n",
    "\n",
    "    long_reviews = shor_reviews[2::4]\n",
    "    short_reviews = shor_reviews[1::4]\n",
    "    short_reviews=short_reviews[:-1]\n",
    "    long_reviews=long_reviews[:-1]\n",
    "\n",
    "    dataframe = pd.DataFrame({'Names':names,'Short_Reviews':short_reviews,'Reviews':long_reviews,'Date':dates,'Color':color})\n",
    "    \n",
    "    dataframe1 = dataframe1.append(dataframe,ignore_index=True)\n",
    "\n",
    "    i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "shor_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
